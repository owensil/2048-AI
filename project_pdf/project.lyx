#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% For algorithm2e
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}
\newcommand{\myname}{Owen Siljander}
%\fxsetup{status=draft}

\title{{\huge TDL Value Function Approximations in {\em 2048}}}
%\author{\myname}
\end_preamble
\use_default_options true
\begin_modules
algorithm2e
customHeadersFooters
fixme
logicalmkup
eqs-within-sections
figs-within-sections
tabs-within-sections
theorems-ams-bytype
theorems-ams-extended-bytype
\end_modules
\maintain_unincluded_children false
\begin_local_layout
PackageOptions algorithm2e ruled
\end_local_layout
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref true
\pdf_title "TDL Value Function Approximations in 2048"
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
Owen Siljander
\end_layout

\begin_layout Standard
\align center
\begin_inset Flex Code
status open

\begin_layout Plain Layout
silja006@umn.edu
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Flex Noun
status open

\begin_layout Plain Layout
University of Minnesota
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsubsection*
Abstract
\end_layout

\begin_layout Standard

\emph on
2048
\emph default
 is a puzzle game released in 2014 by Gabriele Cirulli that quickly gained
 popularity, reaching thousands of years of total playtime by players 
\begin_inset CommandInset citation
LatexCommand cite
key "yeh2016multistage"
literal "false"

\end_inset

.
 The game is particularly attractive as a testing environment for agent
 design due to it being a discrete time stochastic environment with a substantia
l state space.
 Many classical search approaches have been studied under this environment,
 as well as reinforcement learning agents, with substantial success over
 human players 
\begin_inset CommandInset citation
LatexCommand cite
key "yeh2016multistage"
literal "false"

\end_inset

.
 Explicit value functions (e.g.
 table lookup) for temporal difference learning (TDL) are not equipped to
 handle a state space as large as the one in 
\emph on
2048, 
\emph default
and so this paper investigates linear functions of features and neural networks
 as value function approximations for TDL.
\end_layout

\begin_layout Standard
The results of this study are mostly inconclusive in part due to computing
 and time constraints.
 The linear functions of features in experiments would often diverge extremely
 quickly (in less than 10 learning steps) which is perhaps caused by the
 poor convergence of linear approximaters in general 
\begin_inset CommandInset citation
LatexCommand cite
key "sutton2009fast"
literal "false"

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "kolter2009regularization"
literal "false"

\end_inset

.
 Experiments with four three-layered non-linear neural networks showed that
 it occasionally performed better than a simple random agent.
 It is impossible to make a claim in confidence about the performance in
 general due to the insufficient number of games the networks trained for
 and evaluated with.
 The time it takes for this variant to make a decision is comparable to
 classical approaches at around 20s.
 There is large room for improvements and experimentation in the application
 of both these value function approximaters in 
\emph on
2048
\emph default
.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Environment
\end_layout

\begin_layout Standard

\emph on
2048 
\emph default
consists of a 
\begin_inset Formula $4\times4$
\end_inset

 grid starting out with two tiles in random locations with each tile having
 a value of 
\begin_inset Formula $2$
\end_inset

 or 
\begin_inset Formula $4$
\end_inset

 (ex.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2048-starting-state"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 
\begin_inset Float figure
placement h
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/2048_example_1.png
	lyxscale 50
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example starting state for 
\emph on
2048
\begin_inset CommandInset label
LatexCommand label
name "fig:2048-starting-state"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

 The player has the available moves of 
\emph on
up
\emph default
, 
\emph on
down
\emph default
, 
\emph on
left
\emph default
, or 
\emph on
right
\emph default
.
 Each of these actions will slide the tiles in the appropriate direction.
 If a tile collides with or moves into another tile of the same value, they
 combine into a single tile with a value equal to their sum.
 A tile which has been produced through such a combination cannot combine
 with another tile within the same move; that is to say, a player could
 not combine a row of 
\begin_inset Formula $2$
\end_inset

s into a single tile with a value of 
\begin_inset Formula $8$
\end_inset

, it would instead produce two tiles of value 
\begin_inset Formula $4$
\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Some research fails to state this property of the game, it may perhaps be
 implied.
\end_layout

\end_inset

 If a move would not combine or move any tiles on the grid, it is not a
 legal move.
 Each time the agent takes an action, a new tile is generated in a random
 location having a value of 
\begin_inset Formula $2$
\end_inset

 with probability 
\begin_inset Formula $0.9$
\end_inset

 and a value of 
\begin_inset Formula $4$
\end_inset

 with probability 
\begin_inset Formula $0.1$
\end_inset

.
 The game ends when there is no legal move (i.e.
 cannot move or combine tiles).
 For every move, the agent receives a reward equal to the value of all tiles
 which have been combined (e.g.
 combining two 
\begin_inset Formula $2$
\end_inset

 tiles and two 
\begin_inset Formula $4$
\end_inset

 tiles results in a reward of 
\begin_inset Formula $12$
\end_inset

).
 The game is considered to be won when the agent has produced a tile with
 a value of 2048, though the agent can keep playing to achieve higher scores.
\end_layout

\begin_layout Standard
The reason this environment is challenging is not only due to the stochastic
 nature of the game, but also due to the large state space.
 Considering each tile's value is a power of two, and the maximum possible
 tile value (in practice) is 
\begin_inset Formula $2^{17}$
\end_inset

, there are roughly 
\begin_inset Formula $18^{16}\approx10^{20}$
\end_inset

 possible states (a power of 
\begin_inset Formula $18$
\end_inset

 to include the zero/empty tile) 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

 calculates the number of states to be 
\begin_inset Formula $16^{18}=4.7\times10^{21}$
\end_inset

 but I believe this calculation to be erroneous.
\end_layout

\end_inset

 This state space increases greatly when you consider state-action pairs.
 This large state space has made sampling algorithms, such as Monte-Carlo
 tree search (MCTS), particularly attractive solutions.
\end_layout

\begin_layout Subsection
Requisite Knowledge
\end_layout

\begin_layout Standard
This paper focuses on the application of temporal difference learning (TDL)
 in 
\emph on
2048 
\emph default
where the environment is modeled as a Markovian decision process (MDP).
 Temporal difference learning is a particular class of model-free reinforcement
 learning algorithms that use a value function, which calculates the expected
 utility of a state, as a means for decision making.
 By determining the value of potential next states, an agent can use the
 value function to choose the best action to take.
 TDL works by continually adjusting its estimated value function, 
\begin_inset Formula $V^{\pi}(s)$
\end_inset

, under some policy 
\begin_inset Formula $\pi$
\end_inset

.
 Generally, TDL is denoted by TD
\begin_inset Formula $(\lambda)$
\end_inset

 where 
\begin_inset Formula $\lambda$
\end_inset

 denotes the number of states it looks ahead.
 One of the simplest forms is where 
\begin_inset Formula $\lambda=0$
\end_inset

, this actually denotes a one step look-ahead.
 In this case, the value function is updated as shown in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:value_update"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V^{\pi}(s)\leftarrow V^{\pi}(s)+\alpha(r+\gamma V^{\pi}(s^{\prime})-V^{\pi}(s))\label{eq:value_update}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
As for notation, 
\begin_inset Formula $\alpha\in[0,1]$
\end_inset

 is some learning rate, 
\begin_inset Formula $\gamma$
\end_inset

 is the discount factor, 
\begin_inset Formula $s^{\prime}$
\end_inset

 is the state following an action from 
\begin_inset Formula $s$
\end_inset

 under the policy 
\begin_inset Formula $\pi$
\end_inset

, and 
\begin_inset Formula $r$
\end_inset

 is the state reward.
 The value 
\begin_inset Formula $r+\gamma V^{\pi}(s^{\prime})$
\end_inset

 is commonly called the target and denotes the temporal difference between
 the current estimate and the updated estimate.
 The value function additionally satisfies Bellman's equation (Equation
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bellman"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
V^{\pi}(s)=R(s)+\gamma\sum_{s^{\prime}}P(s^{\prime}|s,\pi(s))V^{\pi}(s^{\prime})\label{eq:bellman}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Bellman's equation indicates that the value for a particular state under
 some policy 
\begin_inset Formula $\pi$
\end_inset

 is given by the state reward plus the discounted expected value of all
 possible subsequent states.
 Hence, convergence of such an approximation is not guaranteed and this
 becomes apparent in the results section.
\end_layout

\begin_layout Standard
This paper uses the formal definition of an MDP as found in 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

, an abridged version is repeated here for convenience and completeness.
 An MDP is a tuple 
\begin_inset Formula $\left\langle S,A,R,P\right\rangle $
\end_inset

 where
\end_layout

\begin_layout Itemize
\begin_inset Formula $S$
\end_inset

 is a set of possible states
\end_layout

\begin_layout Itemize
\begin_inset Formula $A$
\end_inset

 is a set of actions, 
\begin_inset Formula $A(s)$
\end_inset

 denotes the possible actions at state 
\begin_inset Formula $s$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $R:S\times A\rightarrow\mathbb{R}$
\end_inset

 is the reward function that maps a state-action pair to a real number
\end_layout

\begin_layout Itemize
\begin_inset Formula $P:S\times A\times S\rightarrow\mathbb{R}$
\end_inset

 is a stochastic transition function where 
\begin_inset Formula $P(s,a,s^{\prime})$
\end_inset

 denotes the probability of transitioning from state 
\begin_inset Formula $s$
\end_inset

 to 
\begin_inset Formula $s^{\prime}$
\end_inset

 with an action 
\begin_inset Formula $a$
\end_inset

.
 
\end_layout

\begin_layout Standard
Because TDL is a model-free algorithm, it isn't necessary for it to have
 a priori knowledge of the transition function or the reward function.
 In some cases, it may be possible to express the reward function explicitly,
 e.g.
 as a lookup table.
 In the case of 
\emph on
2048
\emph default
, the state space makes this solution infeasible.
 Instead, TDL can be modified to use an approximation of the value function.
 In some particular cases, it may be more appropriate to enable decision
 making through a function of state-action pairs (as opposed to just states);
 this is an extension of TDL known as Q-learning.
 Q-learning uses a function 
\begin_inset Formula $Q(s,a):S\times A\rightarrow\mathbb{R}$
\end_inset

 that can discriminate more accurately against actions.
\end_layout

\begin_layout Section
Related Work
\end_layout

\begin_layout Standard
More classical approaches have been taken to create agents for this game,
 namely Minimax, Expectimax, and Monte-Carlo tree search (MCTS).
 For instance, 
\begin_inset CommandInset citation
LatexCommand cite
key "rodgers2014investigation"
literal "false"

\end_inset

 found that averaged depth-limited search (ADLS) and MCTS can exceed average
 scores of 50,0000 and 120,000 respectively.
 However, they found that in particular, these agents worked quite well
 for maintaining high average scores but failed to produce large individual
 scores.
 Specifically, an increase in the search limit for ADLS would reduce the
 number of low-scores, but would also reduce the number of high-scores.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

 used temporal difference learning (TDL) with an 
\begin_inset Formula $n$
\end_inset

-tuple network to create an agent for this game that performed exceedingly
 well.
 In over 97% of plays, the agent reached the 2048 tile and had an average
 score of 
\begin_inset Formula $100,178$
\end_inset

 and a maximum of 
\begin_inset Formula $261,526$
\end_inset

.
 The additional benefit of this technique is that it can make decisions
 far quicker than the more classical approaches.
 In particular, 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

 found that both Expectimax and Minimax performed exceedingly slower.
 The implementation of Expectimax is capable of 6.6 moves per second compared
 to 330,000 moves per second by TDL.
 Nonetheless, TDL still suffers from the same issue as the agents in 
\begin_inset CommandInset citation
LatexCommand cite
key "rodgers2014investigation"
literal "false"

\end_inset

, it struggles to reach large scores as pointed out in 
\begin_inset CommandInset citation
LatexCommand cite
key "yeh2016multistage"
literal "false"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "wu2014multi"
literal "false"

\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "yeh2016multistage"
literal "false"

\end_inset

 proposes an improvement upon the implementation of TDL by 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

; this extension is called multistage temporal difference learning (MS-TD).
 The authors equate it to hierarchical reinforcement learning methods.
 In essence, MS-TD breaks up the training process into three distinct stages,
 each containing their own weights.
 The intuition here is that each stage presents a stronger (and different)
 challenge and is not as forgiving with the agent's actions as it would
 be in the early stages.
 The intuition indeed turned out to be correct, this extension increased
 the rate at which the agent achieved higher scores.
 Further improvements were made upon this to reinforce it with expectimax
 to yield even higher rates of reaching high-valued tiles.
\end_layout

\begin_layout Standard
Some studies have been done using deep reinforcement learning as well, such
 as in 
\begin_inset CommandInset citation
LatexCommand cite
key "dedieu2017deep"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Section
Motivation and Objective
\end_layout

\begin_layout Standard

\emph on
2048
\emph default
 can be modeled as a Markovian Decision Process (MDP); this is a framework
 to facilitate decision making in a (discrete) time stochastic environment.
 The motivation to select TDL for this study is largely arbitrary but is
 also due to the fact that it is a model-free reinforcement learning solution
 to decision making in such an environment.
\end_layout

\begin_layout Standard
While there is a fair amount of research on reinforcement learning and TDL
 for 
\emph on
2048
\emph default
, very few of these studies with TDL appear to discuss ways in which to
 combat the large state space.
 Although these techniques may be sub-optimal compared to the results discussed
 in the related work section, it can be quite illuminating to discover how
 these variants perform and perhaps why.
 TDL is not normally equipped to handle such large state spaces and so this
 paper focuses on particular technique known as value function approximation.
 Of the various techniques for this, this paper will consider linear combination
s of features and neural networks.
 The main objective for studying these techniques is to discover the efficacy
 of value function approximations other than the 
\begin_inset Formula $n$
\end_inset

-tuple networks used in 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
While it is possible to use plain TDL, this paper uses an extension from
 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

 that enables TDL to behave quite like Q-learning.
 Q-learning is a variant upon TDL that considers a state-action value function,
 as opposed to just a state value function in TDL.
 This should increase the accuracy in which the value function approximations
 can classify the expected utility of states.
 For both variants of the value function approximations, there is a distinct
 approximater for each action, denoted 
\begin_inset Formula $V_{a}$
\end_inset

 for the respective action 
\begin_inset Formula $a\in A$
\end_inset

.
 This extension comes from 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

 and is included in this study to reduce the number of variables which may
 cause the results to differ, allowing a more direct comparison of results.
 This extension enables TDL to behave like Q-learning without needing an
 explicit approximater for a state-action value function.
\end_layout

\begin_layout Subsection
Linear Functions of Features
\end_layout

\begin_layout Standard
In direct similarity to 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

, we update the value function approximation according to Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:Update-for-Value"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
SetAlgoLined
\end_layout

\begin_layout Plain Layout


\backslash
DontPrintSemicolon
\end_layout

\begin_layout Plain Layout


\backslash
SetKwFunction{Fun}{Learn-Evaluation}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Fun{
\end_layout

\end_inset


\begin_inset Formula $s,a,r,s',s''$
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

}{
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
Indp
\end_layout

\end_inset


\begin_inset Formula $v_{next}\leftarrow\arg\max_{a^{\prime}\in A(s^{\prime\prime})}V_{a^{\prime}}(s^{\prime\prime})$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $V_{a}(s)\leftarrow V_{a}(s)+\alpha(r+v_{next}-V_{a}(s))$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Update for Value Function Approximation
\begin_inset CommandInset label
LatexCommand label
name "alg:Update-for-Value"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The state succeeding an action is separated into two distinct states: 
\begin_inset Formula $s^{\prime}$
\end_inset

 and 
\begin_inset Formula $s^{\prime\prime}$
\end_inset

.
 
\begin_inset Formula $s^{\prime}$
\end_inset

 denotes the state after an action has taken place but before a tile is
 randomly generated.
 
\begin_inset Formula $s^{\prime\prime}$
\end_inset

 denotes the state after an action and a random tile has been generated.
 This design comes from 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

, though it is not utilized here.
\end_layout

\begin_layout Standard
The value function approximater takes form as
\begin_inset Formula 
\begin{equation}
V_{a}(s)=\sum_{i}w_{ai}^{T}f_{i}(s)=\boldsymbol{w}_{a}^{T}\Phi(s)\label{eq:linear_func}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\boldsymbol{w}_{a}$
\end_inset

 is a weight vector for the value function approximater for action 
\begin_inset Formula $a$
\end_inset

.
 The feature vector, 
\begin_inset Formula $\Phi(s)=\left\langle f_{1}(s),f_{2}(s),\ldots,f_{n}(s)\right\rangle $
\end_inset

, is simply the value of each game tile (e.g.
 
\begin_inset Formula $f_{1}(s)$
\end_inset

 would be the value of the first tile).
 The motivation for selecting this is that a higher weight would be given
 to high value tiles being held in the corner and in adjacent tiles.
 That is, keeping large valued tiles outside of a corner or small valued
 tiles in a corner, would contribute little and would be biased against
 by the value function approximation.
 Keeping high-valued tiles in a corner is a common heuristic used to evaluate
 the quality of states in search based algorithms.
 It's important to note that while the value function satisfies Bellman's
 equation (Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bellman"
plural "false"
caps "false"
noprefix "false"

\end_inset

), approximating the value function in this way is not guaranteed to satisfy
 it 
\begin_inset CommandInset citation
LatexCommand cite
key "kolter2009regularization"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Neural Networks
\end_layout

\begin_layout Standard
Just like in the previous section, four neural networks are used, one for
 each possible action.
 For reasons that will be apparent later, the activation function chosen
 for the neural network is ReLU.
 The optimizer is stochastic gradient descent and each network has three
 layers: a 16 neuron input layer, a middle layer with 4 neurons, and an
 output layer with one neuron.
 The choice for the number of neurons in the middle layer comes from two
 factors: symmetry of the game board, and a common game heuristic.
 A common strategy for 
\emph on
2048
\emph default
 is to keep the highest valued tile in a corner.
 The symmetry of the game board means it does not matter which corner this
 high valued tile is in.
 For this reason, four neurons in the middle layer should pick up on an
 organization of tile values within the four four-tiled corners of the game
 board.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
Results from the experiment were largely inconclusive, and as such, there
 is very little data worth being presented here.
 The information collected from the linear function approximater yielded
 inconclusive results due to the nature of the data.
 The results for neural networks are inconclusive due to the insufficient
 number of training examples (the reasoning for insufficient training is
 discussed later).
 The reasons for such failures and inconclusiveness is nonetheless still
 discussed and various avenues of potential improvement are presented.
\end_layout

\begin_layout Subsection
Linear Functions of Features
\end_layout

\begin_layout Standard
The linear combination of features performed quite poorly and diverged quickly.
 Multiple trials were conducted, both with the weight vectors initialized
 randomly and initialized to zero.
 In all trials, The functions associated with the 
\emph on
left
\emph default
 and 
\emph on
up
\emph default
 moves quickly approached the maximum integer value.
 Interestingly, this type of weighting coincides with a common strategy
 of keeping the high value tile in a corner; which is achievable by repeatedly
 moving 
\emph on
up
\emph default
 and 
\emph on
left 
\emph default
and occasionally 
\emph on
right 
\emph default
or 
\emph on
down 
\emph default
as needed.
 There is not too much to be claimed about this particular result though
 it suggests a more clever feature combination may reveal such a strategy
 is learned by TDL.
 The reason for inconclusiveness is that it is not guaranteed that the linear
 combination of features is even able to approximate the true value function
 
\begin_inset CommandInset citation
LatexCommand cite
key "kolter2009regularization"
literal "false"

\end_inset

 since linear approximations cannot be guaranteed to satisfy the Bellman
 equation.
 This characteristic of linear approximations such as in Q-learning is also
 pointed out in 
\begin_inset CommandInset citation
LatexCommand cite
key "sutton2009fast"
literal "false"

\end_inset

.
 Unfortunately, this discovery was made after the experiment was completed
 and time did not permit exploration of solutions for this.
 Both 
\begin_inset CommandInset citation
LatexCommand cite
key "sutton2009fast"
literal "false"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "kolter2009regularization"
literal "false"

\end_inset

 provide solutions that make convergence a more likely prospect (the former
 being the superior solution of the two).
 Thus, nothing of import can really be ascertained from the results of this
 experiment.
 In theory, the linear function should have no trouble adjusting itself
 to conform to common heuristics.
 
\end_layout

\begin_layout Subsection
Neural Networks
\end_layout

\begin_layout Standard
For the neural networks, these performed quite poorly as well but not much
 can be claimed since the number of games used in training was less than
 a thousand.
 ReLU was chosen as the activation function for the neural networks to introduce
 non-linearity into the function approximation.
 This was largely motivated by the results for the linear combination of
 features being quite poor.
 Training the neural networks adequately would have taken on the order of
 weeks and was thus not feasible given time constraints.
 However, testing the neural networks decision making speeds proved somewhat
 illuminating.
 In comparison to 
\begin_inset CommandInset citation
LatexCommand cite
key "szubert2014temporal"
literal "false"

\end_inset

, which found a game could be completed in 
\begin_inset Formula $23$
\end_inset

ms, the neural networks often took 
\begin_inset Formula $20+$
\end_inset

s to finish a single game.
 Unfortunately, the hardware differences are unknown and thus it is impossible
 to make a claim about the efficacy of neural networks either.
 Nonetheless, even such a small training sample still enabled the agent
 to reach scores over 
\begin_inset Formula $3,000$
\end_inset

, an improvement over random agents which have been shown to never reach
 the 256 tile 
\begin_inset CommandInset citation
LatexCommand cite
key "dedieu2017deep"
literal "false"

\end_inset

.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Of course, given enough trials, a random agent would eventually reach such
 a tile but it's exceedingly rare.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion and Future Work
\end_layout

\begin_layout Standard
The data from experiments was very minimal and not meaningful due to lack
 of training.
 Not much can be claimed about the results from the linear functions, despite
 the experiments running successfully.
 The same holds true for the implementation of neural networks as function
 approximaters, though it is difficult to make claims about the efficacy
 of this solution due to potential hardware differences.
 The neural networks as an approximater nonetheless still achieved better
 results than a random agent.
 This particularly motivates interest into non-linear functions for approximatin
g the value function.
\end_layout

\begin_layout Standard
For future improvements, it would be important to take a closer look at
 linear function approximaters since the results here suggest something
 of interest, though it is not concrete.
 Perhaps improvements to the stability of convergence for linear function
 approximaters as discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "sutton2009fast"
literal "false"

\end_inset

 could be implemented and analyzed to discover particularly why this implementat
ion failed.
 There are many avenues for experimentation with neural networks that time
 did not permit.
 Perhaps the most obvious among these is fine-tuning the layers of the network
 and choosing the activation function.
 It's also possible to use just a single network with an additional input
 neuron containing the action.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "proj_cites"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_body
\end_document
